## 人工智慧的方法
* 比對法
    * 紀錄問題與答案配對後，直接從表格內查出。
    * 例如:Elisa
* 推理法
    * 撰寫規則後，電腦根據規則推論。
    * 例如:專家系統
* 搜尋法
    * 對所有可能的結果進行系統式的列舉，然後看看有沒有答案。
    * 例如:深度優先、廣度優先、電腦下棋
* 統計法
    * 找出機率最大的解答。
    * 例如:利用電腦亂數驗證中央極限定理
* 優化法
    * 對每個可能的解答，都給一個分數及權重，找出總分最好的解答。
    * 例如:爬山演算法、遺傳演算法
## 常見的激活函數
1. Sigmoid
![p](https://github.com/zxc21949049/ai109b/blob/main/pp/ppw1401.png)


Sigmoid函數的特點是會把輸出限定在0~1之間，如果是非常大的負數，輸出就是0，如果是非常大的正數，輸出就是1，這樣使得數據在傳遞過程中不容易發散。

Sigmod有兩個主要缺點，一是Sigmoid容易過飽和，丟失梯度。從Sigmoid的示意圖上可以看到，神經元的活躍度在0和1處飽和，梯度接近於0，這樣在反向傳播時，很容易出現梯度消失的情況，導致訓練無法完整；二是Sigmoid的輸出均值不是0，基於這兩個缺點，SIgmoid使用越來越少了。



2. tanh




![p](https://github.com/zxc21949049/ai109b/blob/main/pp/ppw1402.png)


tanh是Sigmoid函數的變形，tanh的均值是0，在實際應用中有比Sigmoid更好的效果。



3. ReLU


![p](https://github.com/zxc21949049/ai109b/blob/main/pp/ppw1403.png)


ReLU是近來比較流行的激活函數，當輸入信號小於0時，輸出為0；當輸入信號大於0時，輸出等於輸入。



ReLU的優點：

1. ReLU是部分線性的，並且不會出現過飽和的現象，使用ReLU得到的隨機梯度下降法（SGD）的收斂速度比Sigmodi和tanh都快。

2. ReLU只需要一個閾值就可以得到激活值，不需要像Sigmoid一樣需要復雜的指數運算。

ReLU的缺點：

在訓練的過程中，ReLU神經元比價脆弱容易失去作用。例如當ReLU神經元接收到一個非常大的的梯度數據流之後，這個神經元有可能再也不會對任何輸入的數據有反映了，所以在訓練的時候要設置一個較小的合適的學習率參數。



4. Leaky-ReLU


![p](https://github.com/zxc21949049/ai109b/blob/main/pp/ppw1404.png)


相比ReLU，Leaky-ReLU在輸入為負數時引入了一個很小的常數，如0.01，這個小的常數修正了數據分佈，保留了一些負軸的值，在Leaky-ReLU中，這個常數通常需要通過先驗知識手動賦值。



5. Maxout



![p](https://github.com/zxc21949049/ai109b/blob/main/pp/ppw1405.png)



Maxout是在2013年才提出的，是一種激發函數形式，一般情況下如果採用Sigmoid函數的話，在前向傳播過程中，隱含層節點的輸出表達式為：



其中W一般是二維的，這裡表示取出的是第i列，下標i前的省略號表示對應第i列中的所有行。而在Maxout激發函數中，在每一個隱含層和輸入層之間又隱式的添加了一個“隱含層”，這個“隱隱含層”的激活函數是按常規的Sigmoid函數來計算的，而Maxout神經元的激活函數是取得所有這些“隱隱含層”中的最大值，如上圖所示。

Maxout的激活函數表示為：



f( x) = m a x(wT1x+b1,wT2x+
b2)



可以看到，ReLU和Leaky ReLU都是它的一個變形（比如， 的時候，就是ReLU）。

Maxout的擬合能力是非​​常強的，它可以擬合任意的的凸函數，優點是計算簡單，不會過飽和，同時又沒有ReLU的缺點（容易死掉），但Maxout的缺點是過程參數相當於多了一倍。

原文链接：https://blog.csdn.net/dcrmg/article/details/73743742